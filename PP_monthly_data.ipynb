{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from pandas.errors import PerformanceWarning\n",
    "warnings.simplefilter(action='ignore', category=PerformanceWarning)\n",
    "\n",
    "# initialize parameters\n",
    "starting_year_to_filter = 1963\n",
    "end_year_to_filter = 2020\n",
    "number_of_lookback_periods = 120\n",
    "data_to_read_address = [\"data/25_Portfolios_5x5_SizeBM_monthly.csv\",\n",
    "                        \"data/25_Portfolios_5x5_SizeOP_monthly.csv\",\n",
    "                        \"data/25_Portfolios_5x5_SizeINV_monthly.csv\"]\n",
    "factor_data_address = \"data/F-F_Research_Data_5_Factors_2x3.csv\"\n",
    "number_of_PPs_to_consider = 3\n",
    "number_of_PEPs_to_consider = 3\n",
    "number_of_PAPs_to_consider = 3\n",
    "\n",
    "def rank_and_map(df):\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "    # Exclude the 'date' column for ranking\n",
    "    data_columns = df_copy.columns[1:]\n",
    "    \n",
    "    # Apply ranking and scaling row-wise (for each date)\n",
    "    def rank_row(row):\n",
    "        # Get the ranks (min rank is 1)\n",
    "        ranks = row.rank(method='min')\n",
    "        # Normalize the ranks to range between 0 and 1\n",
    "        ranks_normalized = (ranks - 1) / (len(row) - 1)\n",
    "        # Map to range [-0.5, 0.5]\n",
    "        return ranks_normalized - 0.5\n",
    "    \n",
    "    # Apply rank_row function to each row, excluding the 'date' column\n",
    "    df_copy[data_columns] = df_copy[data_columns].apply(rank_row, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def cross_sectional_demean(df):\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "    # Exclude the 'date' column\n",
    "    data_columns = df_copy.columns[1:]\n",
    "    \n",
    "    # Apply demeaning row-wise (for each date)\n",
    "    def demean_row(row):\n",
    "        row_mean = row.mean()  # Compute the mean of the row\n",
    "        return row - row_mean  # Subtract the mean from each element in the row\n",
    "    \n",
    "    # Apply demean_row function to each row, excluding the 'date' column\n",
    "    df_copy[data_columns] = df_copy[data_columns].apply(demean_row, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def compute_rs_product(df1, df2):\n",
    "    # Ensure the date columns match\n",
    "    if not df1['date'].equals(df2['date']):\n",
    "        raise ValueError(\"Date columns of both dataframes must match.\")\n",
    "    \n",
    "  # Convert to numeric, set invalid values as NaN\n",
    "    df1 = df1.astype({col: 'float64' for col in df1.columns if col != 'date'})\n",
    "    df2 = df2.astype({col: 'float64' for col in df2.columns if col != 'date'})\n",
    "    result = {}\n",
    "    \n",
    "    # Iterate over each row (each date)\n",
    "    for index, date in enumerate(df1['date']):\n",
    "        # Get the R vector (from df1) and S' vector (from df2) for the current date\n",
    "        R = df1.iloc[index, 1:].values.reshape(-1, 1)  # n x 1 vector\n",
    "        S_transpose = df2.iloc[index, 1:].values.reshape(1, -1)  # 1 x n vector\n",
    "        # Compute the outer product (RS')\n",
    "        matrix_rs = np.dot(R, S_transpose)  # n x n matrix\n",
    "        # Store the result in a dictionary, with date as the key\n",
    "        result[date] = matrix_rs\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_prediction_matrix(input_date, result_matrices, n_periods):\n",
    "    # Sort the dates in result_matrices to ensure they're in order\n",
    "    sorted_dates = sorted(result_matrices.keys())\n",
    "    # Find the index of the input date in the sorted list of dates\n",
    "    if input_date not in sorted_dates:\n",
    "        raise ValueError(\"The input date is not found in the result_matrices.\")\n",
    "    \n",
    "    input_date_index = sorted_dates.index(input_date)\n",
    "    # Select the last n_periods (excluding the input date)\n",
    "    start_index = max(0, input_date_index - n_periods)  # Ensure we don't go below index 0\n",
    "    selected_dates = sorted_dates[start_index:input_date_index]\n",
    "    \n",
    "    if len(selected_dates) == 0:\n",
    "        raise ValueError(f\"There are no previous periods to calculate the average for the given number: {n_periods}.\")\n",
    "    \n",
    "    # Initialize a matrix to accumulate the sum\n",
    "    matrix_shape = result_matrices[sorted_dates[0]].shape\n",
    "    sum_matrix = np.zeros(matrix_shape, dtype=float)\n",
    "    # Sum all the selected matrices\n",
    "    for date in selected_dates:\n",
    "        sum_matrix += np.array(result_matrices[date], dtype=float)\n",
    "    \n",
    "    # Calculate the element-wise average\n",
    "    average_matrix = sum_matrix / len(selected_dates)\n",
    "    return average_matrix\n",
    "\n",
    "\n",
    "# i should start from 0. In other words, to get the first PP's expected return you must set i=0.\n",
    "def get_ith_PPs_expected_return(S,i):\n",
    "    return S[i]\n",
    "\n",
    "# i should start from 0. In other words, to get the first PP you must set i=0.\n",
    "def get_ith_position_matrix(U,VT,i):\n",
    "    u_column = U[:, i]\n",
    "    v_column = VT[i, :]\n",
    "    return np.outer(v_column,u_column)\n",
    "\n",
    "def first_n_PPs_expected_return(S,n):\n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        sum += get_ith_PPs_expected_return(S,i)\n",
    "    return sum\n",
    "\n",
    "def first_n_PPs_position_matrix(U,VT,number_of_PPs):\n",
    "    matrix_shape = U.shape\n",
    "    sum_matrix = np.zeros(matrix_shape, dtype=float)\n",
    "    for i in range(number_of_PPs):\n",
    "        sum_matrix += get_ith_position_matrix(U,VT,i)\n",
    "    return sum_matrix/number_of_PPs\n",
    "\n",
    "\n",
    "# i should start from 0. In other words, to get the first PEP you must set i=0.\n",
    "def get_ith_PEPs_expected_return(eigenvalues,i):\n",
    "    return eigenvalues[i]\n",
    "\n",
    "def get_ith_symmetric_position_matrix(eigenvectors,i):\n",
    "    w = eigenvectors[:, i]\n",
    "    return np.outer(w,w)\n",
    "\n",
    "def first_n_PEPs_expected_return(eigenvalues,n):\n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        sum += abs(get_ith_PEPs_expected_return(eigenvalues,i))\n",
    "    return sum\n",
    "\n",
    "def first_n_PEPs_position_matrix(eigenvectors,number_of_PEPs):\n",
    "    matrix_shape = eigenvectors.shape\n",
    "    sum_matrix = np.zeros(matrix_shape, dtype=float)\n",
    "    for i in range(number_of_PEPs):\n",
    "        sum_matrix += get_ith_symmetric_position_matrix(eigenvectors,i)\n",
    "    return sum_matrix/number_of_PEPs\n",
    "\n",
    "# i should start from 0. In other words, to get the first PEP you must set i=0.\n",
    "def get_ith_PAPs_expected_return(filtered_eigenvalues_ta,i):\n",
    "    return 2 * filtered_eigenvalues_ta[i]\n",
    "\n",
    "def get_ith_asymmetric_position_matrix(sorted_eigenvectors_ta_real_part,sorted_eigenvectors_ta_imaginary_part,i):\n",
    "    return np.outer(sorted_eigenvectors_ta_real_part[:,i],sorted_eigenvectors_ta_imaginary_part[:,i]) - np.outer(sorted_eigenvectors_ta_imaginary_part[:,i],sorted_eigenvectors_ta_real_part[:,i])\n",
    "    \n",
    "def first_n_PAPs_expected_return(filtered_eigenvalues_ta,n):\n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        sum += get_ith_PAPs_expected_return(filtered_eigenvalues_ta,i)\n",
    "    return sum\n",
    "\n",
    "def first_n_PAPs_position_matrix(sorted_eigenvectors_ta_real_part,sorted_eigenvectors_ta_imaginary_part,number_of_PAPs):\n",
    "    number_of_rows = sorted_eigenvectors_ta_real_part.shape[0]\n",
    "    sum_matrix = np.zeros((number_of_rows,number_of_rows), dtype=float)\n",
    "    for i in range(number_of_PAPs):\n",
    "        sum_matrix += get_ith_asymmetric_position_matrix(sorted_eigenvectors_ta_real_part,sorted_eigenvectors_ta_imaginary_part,i)\n",
    "    return sum_matrix/number_of_PAPs\n",
    "\n",
    "def calculate_sharpe_ratio(returns):\n",
    "    # Compute excess returns\n",
    "    \n",
    "    # Compute average excess return\n",
    "    average_return = returns.mean()\n",
    "    \n",
    "    # Compute standard deviation of returns\n",
    "    std_dev_returns = returns.std()\n",
    "    \n",
    "    # Compute Sharpe Ratio\n",
    "    sharpe_ratio = average_return / std_dev_returns\n",
    "    \n",
    "    return sharpe_ratio\n",
    "\n",
    "def filter_dataframes_by_common_dates(df1, df2):\n",
    "    # Find common dates (intersection of index values)\n",
    "    common_dates = df1.index.intersection(df2.index)\n",
    "    \n",
    "    # Filter both dataframes to keep only the rows with the common dates\n",
    "    df1_filtered = df1.loc[common_dates]\n",
    "    df2_filtered = df2.loc[common_dates]\n",
    "    \n",
    "    return df1_filtered, df2_filtered\n",
    "\n",
    "\n",
    "def regression_results(X, Y):\n",
    "    # Get the standard deviation of X[1] (which is the first non-constant column of X)\n",
    "    std_X1 = X.iloc[:, 1].std()  # Assuming X[1] is the first non-constant column\n",
    "\n",
    "    # Scale Y to have the same standard deviation as X[1]\n",
    "    std_Y = Y.std()\n",
    "    Y_scaled = Y * (std_X1 / std_Y)  # Scale Y by the ratio of std_X1 to std_Y\n",
    "\n",
    "    # Add a constant (intercept) to the independent variables\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the model\n",
    "    model = sm.OLS(Y_scaled, X).fit()\n",
    "\n",
    "    # Get the coefficients (including the intercept)\n",
    "    coefficients = model.params.values\n",
    "\n",
    "\n",
    "    # Get the t-statistics\n",
    "    t_stats = model.tvalues.values\n",
    "\n",
    "    # Calculate the standard deviation of residuals\n",
    "    residuals = model.resid\n",
    "    std_residuals = residuals.std(ddof=1)  # Sample standard deviation (ddof=1)\n",
    "\n",
    "    # Calculate intercept divided by standard deviation of residuals\n",
    "    intercept_over_std_residuals = (coefficients[0] / std_residuals) * math.sqrt(12) #sqrt(12) is to make the IR annualized. again, to make comparison with Kelly's paper easier.\n",
    "\n",
    "    # Get the R-squared value\n",
    "    r_squared = model.rsquared\n",
    "\n",
    "    coefficients[0] = coefficients[0] * 12 # This is to make the results comparable to Kelly(2022). They report annualized alphas in their tables.\n",
    "\n",
    "    # Get the names of the independent variables (including the constant)\n",
    "    variable_names = X.columns.tolist()\n",
    "\n",
    "    # Return the list as required\n",
    "    return [coefficients, t_stats, intercept_over_std_residuals, r_squared, variable_names]\n",
    "\n",
    "\n",
    "\n",
    "def build_PP(data_to_read_address, factor_data_address):\n",
    "    \n",
    "    # Reading FF factor data\n",
    "    factor_data_monthly = pd.read_csv(factor_data_address)\n",
    "    factor_data_monthly['date'] = pd.to_datetime(factor_data_monthly['date'], format='%Y%m') + pd.offsets.MonthEnd(1)\n",
    "    factor_data_monthly = factor_data_monthly.set_index(\"date\")\n",
    "\n",
    "    # Reading asset returns.\n",
    "    portfolios_dataset_df = pd.read_csv(data_to_read_address)\n",
    "    portfolios_dataset_df['date'] = pd.to_datetime(portfolios_dataset_df['date'], format='%Y%m') + pd.offsets.MonthEnd(1)\n",
    "    \n",
    "    signal_df = pd.DataFrame()\n",
    "    signal_df[\"date\"] = portfolios_dataset_df[\"date\"]\n",
    "    # Note that I shift signals one period forward to make computations easier. \n",
    "    signal_df= signal_df.join(portfolios_dataset_df.iloc[:, 1:].shift(1))\n",
    "    # I can think of this matrix as $S_{t-1}$.\n",
    "    normalized_signal_df = rank_and_map(signal_df)\n",
    "    normalized_signal_df = normalized_signal_df[(normalized_signal_df['date'].dt.year > starting_year_to_filter) & (normalized_signal_df['date'].dt.year < end_year_to_filter)].reset_index(drop=True)\n",
    "    # This matrix can be denoted as $R_{t-1}$\n",
    "    demeaned_return_df = cross_sectional_demean(portfolios_dataset_df)\n",
    "    demeaned_return_df = demeaned_return_df[(demeaned_return_df['date'].dt.year > starting_year_to_filter) & (demeaned_return_df['date'].dt.year < end_year_to_filter)].reset_index(drop=True)\n",
    "    # This gives: $R_{t}S'_{t}$\n",
    "    rs_matrix = compute_rs_product(demeaned_return_df, normalized_signal_df)\n",
    "    \n",
    "    \"\"\"\n",
    "    Prediction matrix for date T+1, used returns data up to month T and signals data up to month T-1.\n",
    "    In the function get_prediction_matrix, I start the calculations from the previous month. \n",
    "    Note that although the input date is the current data, but the in the function that month is excluded.\n",
    "    Note that in calculating realized returns, I am using the current month(the month of rearlized returns) as index. \n",
    "    But remember that the matrix was $S_{t-1}$. So, the index actually retreives the value of the previous month. \n",
    "    I formed the matrix this way in order to make the calculations easier.\n",
    "    \"\"\"\n",
    "\n",
    "    realized_returns_df = pd.DataFrame(columns=[\n",
    "        \"date\",\n",
    "        \"return_of_simple_factor\", \n",
    "        \"realized_return_of_first_three_PP\", \n",
    "        \"expected_return_of_first_three_PP\",\n",
    "        \"realized_return_of_first_three_PEP\",\n",
    "        \"expected_return_of_first_three_PEP\",\n",
    "        \"realized_return_of_first_three_PAP\",\n",
    "        \"expected_return_of_first_three_PAP\"\n",
    "    ])\n",
    "\n",
    "    # I leave out the first 120 (number of lookback periods) observations to compute the prediction matrix.\n",
    "    for date_index in demeaned_return_df.iloc[number_of_lookback_periods:]['date']:\n",
    "        date_to_consider = pd.Timestamp(date_index)\n",
    "        \n",
    "        #for PP's\n",
    "        prediction_matrix = get_prediction_matrix(date_to_consider, rs_matrix, number_of_lookback_periods)\n",
    "        U, S, VT = np.linalg.svd(prediction_matrix)\n",
    "\n",
    "        #for PEP's\n",
    "        Symmetric_prediction_matrix = (prediction_matrix + prediction_matrix.T)/2\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(Symmetric_prediction_matrix)\n",
    "        idx = eigenvalues.argsort()[::-1]  # Sort in descending order\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # for PAP'S\n",
    "        assymetric_prediction_matrix = 0.5 * (prediction_matrix - prediction_matrix.T)\n",
    "        transposed_assymetric_prediction_matrix = assymetric_prediction_matrix.T\n",
    "        eigenvalues_ta, eigenvectors_ta = np.linalg.eig(transposed_assymetric_prediction_matrix)\n",
    "        sorted_indices_ta = np.argsort(-eigenvalues_ta.imag)\n",
    "        sorted_eigenvalues_ta = eigenvalues_ta[sorted_indices_ta].imag\n",
    "        sorted_eigenvectors_ta = eigenvectors_ta[:, sorted_indices_ta] * math.sqrt(2)  #sqrt(2) is to make the size of the vectors equal to 1.\n",
    "        positive_indices = np.where(sorted_eigenvalues_ta > 0)\n",
    "        filtered_eigenvalues_ta = sorted_eigenvalues_ta[positive_indices]\n",
    "        filtered_eigenvectors_ta = sorted_eigenvectors_ta[:, positive_indices].squeeze()\n",
    "        sorted_eigenvectors_ta_imaginary_part = filtered_eigenvectors_ta.imag\n",
    "        sorted_eigenvectors_ta_real_part = filtered_eigenvectors_ta.real\n",
    "\n",
    "        #to calculate realized returns\n",
    "        signal_vector = normalized_signal_df[normalized_signal_df.date == date_to_consider].values[0, 1:].reshape(1, -1)  # 1*n matrix\n",
    "        return_vector = portfolios_dataset_df[portfolios_dataset_df.date == date_to_consider].values[0, 1:].reshape(-1, 1)  # n*1  # there is not much difference between using demeaned returns or not_demeaned ones. I can replace portfolios_dataset_df with demeaned_return_df.\n",
    "        \n",
    "\n",
    "        # Compute realized returns\n",
    "        return_of_simple_factor = (signal_vector @ return_vector)[0][0]\n",
    "        realized_return_of_first_three_PP = (signal_vector @ first_n_PPs_position_matrix(U, VT, number_of_PPs_to_consider) @ return_vector)[0][0]\n",
    "        expected_return_of_first_three_PP = first_n_PPs_expected_return(S, number_of_PPs_to_consider)\n",
    "        realized_return_of_first_three_PEP = (signal_vector @ first_n_PEPs_position_matrix(eigenvectors,number_of_PEPs_to_consider) @ return_vector)[0][0]\n",
    "        expected_return_of_first_three_PEP = first_n_PEPs_expected_return(eigenvalues, number_of_PEPs_to_consider)\n",
    "        realized_return_of_first_three_PAP = (signal_vector @ first_n_PAPs_position_matrix(sorted_eigenvectors_ta_real_part,sorted_eigenvectors_ta_imaginary_part,number_of_PAPs_to_consider) @ return_vector)[0][0]\n",
    "        expected_return_of_first_three_PAP = first_n_PAPs_expected_return(filtered_eigenvalues_ta, number_of_PAPs_to_consider)\n",
    "        \n",
    "\n",
    "        # Prepare a list for the current row values\n",
    "        row_values = [\n",
    "            date_index,\n",
    "            return_of_simple_factor,  \n",
    "            realized_return_of_first_three_PP, \n",
    "            expected_return_of_first_three_PP,\n",
    "            realized_return_of_first_three_PEP,\n",
    "            expected_return_of_first_three_PEP,\n",
    "            realized_return_of_first_three_PAP,\n",
    "            expected_return_of_first_three_PAP\n",
    "        ]\n",
    "\n",
    "        # Iterate over all Principal Portfolios (up to len(S)) and calculate realized/expected returns for each\n",
    "        for i in range(len(S)):\n",
    "\n",
    "            # for PP's\n",
    "            realized_return_ith_PP = (signal_vector @ get_ith_position_matrix(U, VT, i) @ return_vector)[0][0]\n",
    "            expected_return_ith_PP = get_ith_PPs_expected_return(S, i)\n",
    "            # Add the values for realized and expected returns of the ith PP to the row\n",
    "            row_values.append(realized_return_ith_PP)\n",
    "            row_values.append(expected_return_ith_PP)\n",
    "\n",
    "            # for PEP's\n",
    "            realized_return_ith_PEP = (signal_vector @ get_ith_symmetric_position_matrix(eigenvectors, i) @ return_vector)[0][0]\n",
    "            expected_return_ith_PEP = get_ith_PEPs_expected_return(eigenvalues, i)\n",
    "            # Add the values for realized and expected returns of the ith PEP to the row\n",
    "            row_values.append(realized_return_ith_PEP)\n",
    "            row_values.append(expected_return_ith_PEP)\n",
    "\n",
    "\n",
    "            # Dynamically add columns if they don't exist. for PP's.\n",
    "            realized_col_name_pp = f\"realized_return_of_{i+1}_PP\"\n",
    "            expected_col_name_pp = f\"expected_return_of_{i+1}_PP\"\n",
    "\n",
    "            # Dynamically add columns if they don't exist. for PEP's.\n",
    "            realized_col_name_pep = f\"realized_return_of_{i+1}_PEP\"\n",
    "            expected_col_name_pep = f\"expected_return_of_{i+1}_PEP\"\n",
    "            \n",
    "            # for PP's\n",
    "            if realized_col_name_pp not in realized_returns_df.columns:\n",
    "                realized_returns_df[realized_col_name_pp] = None\n",
    "            if expected_col_name_pp not in realized_returns_df.columns:\n",
    "                realized_returns_df[expected_col_name_pp] = None\n",
    "\n",
    "            #for PEP'S\n",
    "            if realized_col_name_pep not in realized_returns_df.columns:\n",
    "                realized_returns_df[realized_col_name_pep] = None\n",
    "            if expected_col_name_pep not in realized_returns_df.columns:\n",
    "                realized_returns_df[expected_col_name_pep] = None\n",
    "                \n",
    "        # The number of PAP's is different, so another loop is needed.\n",
    "        for i in range(sorted_eigenvectors_ta_imaginary_part.shape[1]):\n",
    "            # for PAP's\n",
    "            realized_return_ith_PAP = (signal_vector @ get_ith_asymmetric_position_matrix(sorted_eigenvectors_ta_real_part,sorted_eigenvectors_ta_imaginary_part,i) @ return_vector)[0][0]\n",
    "            expected_return_ith_PAP = get_ith_PAPs_expected_return(filtered_eigenvalues_ta,i)\n",
    "            # Add the values for realized and expected returns of the ith PEP to the row\n",
    "            row_values.append(realized_return_ith_PAP)\n",
    "            row_values.append(expected_return_ith_PAP)\n",
    "\n",
    "            # Dynamically add columns if they don't exist. for PEP's.\n",
    "            realized_col_name_pap = f\"realized_return_of_{i+1}_PAP\"\n",
    "            expected_col_name_pap = f\"expected_return_of_{i+1}_PAP\"\n",
    "\n",
    "            #for PAP'S\n",
    "            if realized_col_name_pap not in realized_returns_df.columns:\n",
    "                realized_returns_df[realized_col_name_pap] = None\n",
    "            if expected_col_name_pap not in realized_returns_df.columns:\n",
    "                realized_returns_df[expected_col_name_pap] = None\n",
    "\n",
    "        # Append the row to the dataframe\n",
    "        realized_returns_df.loc[len(realized_returns_df)] = row_values\n",
    "\n",
    "    realized_returns_df = realized_returns_df.set_index(\"date\")\n",
    "\n",
    "    pap_std = realized_returns_df['realized_return_of_first_three_PAP'].std()\n",
    "    pep_std = realized_returns_df['realized_return_of_first_three_PEP'].std()\n",
    "    realized_returns_df['adjusted_PAP'] = realized_returns_df['realized_return_of_first_three_PAP'] * (pep_std / pap_std)\n",
    "    # Take the average of the adjusted \"PAP\" and \"PEP\"\n",
    "    realized_returns_df['PEP and PAP 1-3'] = (realized_returns_df['adjusted_PAP'] + realized_returns_df['realized_return_of_first_three_PEP']) / 2\n",
    "    # drop the adjusted column.\n",
    "    realized_returns_df.drop(columns='adjusted_PAP', inplace=True)\n",
    "\n",
    "    sharpe_df = realized_returns_df.drop(realized_returns_df.filter(like=\"expected\").columns, axis=1).apply(lambda col: calculate_sharpe_ratio(col)) * math.sqrt(12)\n",
    "\n",
    "    pp_columns = realized_returns_df.filter(like=\"PP\")\n",
    "    pp_realized_mean_df = pp_columns.filter(like=\"realized\").mean(axis=0)\n",
    "    pp_expected_mean_df = pp_columns.filter(like=\"expected\").mean(axis=0)\n",
    "\n",
    "    pep_columns = realized_returns_df.filter(like=\"PEP\")\n",
    "    pep_realized_mean_df = pep_columns.filter(like=\"realized\").mean(axis=0)\n",
    "    pep_expected_mean_df = pep_columns.filter(like=\"expected\").mean(axis=0)\n",
    "\n",
    "    pap_columns = realized_returns_df.filter(like=\"PAP\")\n",
    "    pap_realized_mean_df = pap_columns.filter(like=\"realized\").mean(axis=0)\n",
    "    pap_expected_mean_df = pap_columns.filter(like=\"expected\").mean(axis=0)\n",
    "\n",
    "    output_dict = {\n",
    "    'realized_returns_df': realized_returns_df,\n",
    "    'sharpe_df': sharpe_df,\n",
    "    'pp_realized_mean_df': pp_realized_mean_df,\n",
    "    'pp_expected_mean_df': pp_expected_mean_df,\n",
    "    'pep_realized_mean_df': pep_realized_mean_df,\n",
    "    'pep_expected_mean_df': pep_expected_mean_df,\n",
    "    'pap_realized_mean_df': pap_realized_mean_df,\n",
    "    'pap_expected_mean_df': pap_expected_mean_df\n",
    "    }\n",
    "\n",
    "    \n",
    "    realized_returns_df, factor_data_monthly = filter_dataframes_by_common_dates(realized_returns_df,factor_data_monthly)\n",
    "    X = factor_data_monthly[['Mkt-RF', 'SMB','HML','RMW','CMA']]\n",
    "    Y = realized_returns_df['return_of_simple_factor']\n",
    "    output_dict[\"regression_result_return_of_simple_factor\"] = regression_results(X,Y)\n",
    "\n",
    "    # Following Kelly's paper. For regressions that have PP's as Y, simple factor must be addes as X.\n",
    "    X = pd.concat([X, realized_returns_df['return_of_simple_factor']], axis=1)\n",
    "    # List of column names in realized_returns_df for which I want to perform the regression\n",
    "    columns_to_iterate = ['realized_return_of_first_three_PP',\n",
    "                      'realized_return_of_first_three_PEP',\n",
    "                      'realized_return_of_first_three_PAP',\n",
    "                       'PEP and PAP 1-3'] \n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in columns_to_iterate:\n",
    "        # Set Y to the column from realized_returns_df\n",
    "        Y = realized_returns_df[col]\n",
    "        # Store the results with a variable name based on the column name\n",
    "        result_name = f'regression_result_{col}'\n",
    "        output_dict[result_name] = regression_results(X,Y)\n",
    "    return output_dict  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "for dataset in data_to_read_address:\n",
    "    output_dict[dataset.split(\"/\")[1].split(\".csv\")[0]] = build_PP(dataset,factor_data_address)\n",
    "# I calculate the results for all datasets. the results are stored in this dictionay.\n",
    "print(output_dict.keys())\n",
    "'''\n",
    "Every value of this dictionary is itself another dictionary.\n",
    "For instance the key 'sharpe_df' can be used to access the sharp values of this set of results.\n",
    "'''\n",
    "print(output_dict['25_Portfolios_5x5_SizeBM_monthly'].keys())\n",
    "# output_dict['25_Portfolios_5x5_SizeBM_monthly'][\"sharpe_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['25_Portfolios_5x5_SizeBM_monthly', '25_Portfolios_5x5_SizeOP_monthly', '25_Portfolios_5x5_SizeINV_monthly'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['realized_returns_df', 'sharpe_df', 'pp_realized_mean_df', 'pp_expected_mean_df', 'pep_realized_mean_df', 'pep_expected_mean_df', 'pap_realized_mean_df', 'pap_expected_mean_df', 'regression_result_return_of_simple_factor', 'regression_result_realized_return_of_first_three_PP', 'regression_result_realized_return_of_first_three_PEP', 'regression_result_realized_return_of_first_three_PAP', 'regression_result_PEP and PAP 1-3'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
